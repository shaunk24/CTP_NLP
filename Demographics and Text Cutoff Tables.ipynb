{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = pd.read_csv('IAD With Hist and Demos (120)', index_col=0,header=0)\n",
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in DATA.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def numeric_data(data):\n",
    "    data = data[~pd.isna(data)]\n",
    "    stat, p_value = stats.shapiro(data)\n",
    "    print('Shapiro-Wilk Test: Statistics=%.2f, p=%.2f' % (stat, p_value))\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        print(\"Data is normally distributed (fail to reject H0)\")\n",
    "        mean = np.mean(data)\n",
    "        std_dev = np.std(data)\n",
    "        print(f\"Mean: {mean:.2f}, Standard Deviation: {std_dev:.2f}\")\n",
    "    else:\n",
    "        print(\"Data is not normally distributed (reject H0)\")\n",
    "        median = np.median(data)\n",
    "        lqr = np.percentile(data, 25)\n",
    "        hqr = np.percentile(data, 75)\n",
    "        print(f\"Median: {median:.2f}, Interquartile Range (IQR): [{lqr:.2f}-{hqr:.2f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(DATA['AgeAsofScan'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Sex_Female'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Sex_Male'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'SecondRace_Asian'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'SecondRace_Black or African-American'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'SecondRace_White'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'SecondRace_Unknown/Not reported'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(DATA['comorbidity_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comorbidities = pd.read_csv('Comorbidities (80)', index_col=0, header=0)\n",
    "comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in comorbidities:\n",
    "    if a == 'id':\n",
    "        continue\n",
    "    print(a)\n",
    "    print(comorbidities[a].sum())\n",
    "    print(comorbidities[a].sum()/comorbidities.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OS_DATA = pd.get_dummies(DATA[['Occlusion Location']])\n",
    "OS_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Occlusion Location_ICA'\n",
    "print(var + f\": {OS_DATA[var].sum()} ({OS_DATA[var].sum()/OS_DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Occlusion Location_ACA'\n",
    "print(var + f\": {OS_DATA[var].sum()} ({OS_DATA[var].sum()/OS_DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Occlusion Location_MCA'\n",
    "print(var + f\": {OS_DATA[var].sum()} ({OS_DATA[var].sum()/OS_DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Occlusion Location_Basilar/top vertebral'\n",
    "print(var + f\": {OS_DATA[var].sum()} ({OS_DATA[var].sum()/OS_DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Occlusion Location_PCA'\n",
    "print(var + f\": {OS_DATA[var].sum()} ({OS_DATA[var].sum()/OS_DATA.shape[0]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHSS = pd.read_csv('NIHSS data (127)',index_col=0, header=0)\n",
    "NIHSS['PrimaryMrn'] = NIHSS['PrimaryMrn'].map(str)\n",
    "NIHSS = NIHSS[NIHSS['PrimaryMrn'].isin(DATA['PatientID'])]\n",
    "NIHSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(NIHSS['FlowsheetNIHSS_Score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(DATA['LKWDiff'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAD = pd.read_csv('Included Aggregate Data (120)')\n",
    "IAD['FSDiff'] = pd.to_datetime(IAD['ScanTime']) - pd.to_datetime(IAD['FirstScanTime'])\n",
    "IAD['FSDiff'] = IAD['FSDiff'].astype(int) * 1.6667e-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(IAD['FSDiff'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Notes = pd.read_excel('/Users/shaunkohli/Desktop/Kummer Project/Most Recent Note Model (9-28-23)/CTP_ClarityNotes_N=151_v3.xlsx')\n",
    "Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for id,scantime in IAD[['PatientID','ScanTime']].values:\n",
    "    sub_Notes = Notes[Notes['PAT_MRN_ID']==id]\n",
    "    sub_Notes['Entry DateTime'] = sub_Notes['ENTRY_INST_LOCAL_DTTM'].apply(pd.to_datetime)\n",
    "    sub_Notes = sub_Notes[scantime < sub_Notes['Entry DateTime']]\n",
    "    sub_Notes = sub_Notes[sub_Notes['CNCT_NOTE_TYPE_NAME'] == 'Brief Op Note']\n",
    "    if sub_Notes.shape[0] > 0:\n",
    "        count += 1\n",
    "    \n",
    "print(count)\n",
    "print(count/DATA.shape[0])\n",
    "print(DATA.shape[0]-count)\n",
    "print((DATA.shape[0]-count)/DATA.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(DATA['>6s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data(DATA['<30%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'P:C(30)>=1.8'\n",
    "print(var + f\": {DATA[var].sum()} ({DATA[var].sum()/DATA.shape[0]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 500\n",
    "num_notes = []\n",
    "time_difs = []\n",
    "word_counts = []\n",
    "char_counts = []\n",
    "\n",
    "for id,scantime in IAD[['PatientID','ScanTime']].values:\n",
    "    Notes_included = Notes[Notes['PAT_MRN_ID']== str(id)]\n",
    "    Notes_included['Entry DateTime'] = Notes_included['ENTRY_INST_LOCAL_DTTM'].apply(pd.to_datetime)\n",
    "    scantime = pd.to_datetime(scantime)\n",
    "    Notes_included = Notes_included[scantime > Notes_included['Entry DateTime']]\n",
    "    Notes_included = Notes_included[scantime - Notes_included['Entry DateTime'] < pd.Timedelta('1w')]\n",
    "    merged_notes = Notes_included.groupby('NOTE_ID')['NOTE_TEXT'].agg(' '.join)\n",
    "    if merged_notes.shape[0] == 0:\n",
    "        continue\n",
    "    Notes_included = pd.merge(Notes_included, merged_notes, on='NOTE_ID',how='left')\n",
    "    Notes_included.drop('NOTE_TEXT_x',axis=1,inplace=True)\n",
    "    Notes_included.drop_duplicates('NOTE_TEXT_y',inplace=True)\n",
    "    Notes_included['line length'] = Notes_included['NOTE_TEXT_y'].apply(len)\n",
    "    Notes_included = Notes_included.iloc[::-1]\n",
    "    Notes_included['length sum'] = Notes_included['line length'].cumsum()\n",
    "    Notes_included['met threshold?'] = Notes_included['length sum'] >= cutoff\n",
    "    if Notes_included['met threshold?'].sum() == 0:\n",
    "        row_index = Notes_included.shape[0]\n",
    "    else:\n",
    "        row_index = Notes_included['met threshold?'].argmax()\n",
    "    num_notes.append(row_index+1)\n",
    "    Notes_included = Notes_included.iloc[:row_index+1]\n",
    "    Notes_included['time_dif'] = scantime - Notes_included['Entry DateTime']\n",
    "    Notes_included['word_count'] = Notes_included['NOTE_TEXT_y'].apply(lambda text: len(text.split()))\n",
    "    word_counts.append(Notes_included['word_count'].sum())\n",
    "    char_counts.append(Notes_included['line length'].sum())\n",
    "    time_difs.append(Notes_included['time_dif'])\n",
    "\n",
    "time_difs = pd.concat(time_difs)\n",
    "minutes = time_difs.dt.total_seconds()/60\n",
    "minutes.sort_values(inplace=True)\n",
    "\n",
    "print('Median minutes between included notes and scan time [IQR]')\n",
    "print(np.percentile(minutes,25))\n",
    "print(np.percentile(minutes,50))\n",
    "print(np.percentile(minutes,75))\n",
    "\n",
    "plt.hist(word_counts,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median word count in included text [IQR]')\n",
    "print(np.percentile(word_counts,25))\n",
    "print(np.percentile(word_counts,50))\n",
    "print(np.percentile(word_counts,75))\n",
    "\n",
    "plt.hist(char_counts,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median char count in included text [IQR]')\n",
    "print(np.percentile(char_counts,25))\n",
    "print(np.percentile(char_counts,50))\n",
    "print(np.percentile(char_counts,75))\n",
    "\n",
    "plt.hist(num_notes,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median num notes count in included text [IQR]')\n",
    "print(np.percentile(num_notes,25))\n",
    "print(np.percentile(num_notes,50))\n",
    "print(np.percentile(num_notes,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1000\n",
    "num_notes = []\n",
    "time_difs = []\n",
    "word_counts = []\n",
    "char_counts = []\n",
    "\n",
    "for id,scantime in IAD[['PatientID','ScanTime']].values:\n",
    "    Notes_included = Notes[Notes['PAT_MRN_ID']== str(id)]\n",
    "    Notes_included['Entry DateTime'] = Notes_included['ENTRY_INST_LOCAL_DTTM'].apply(pd.to_datetime)\n",
    "    scantime = pd.to_datetime(scantime)\n",
    "    Notes_included = Notes_included[scantime > Notes_included['Entry DateTime']]\n",
    "    Notes_included = Notes_included[scantime - Notes_included['Entry DateTime'] < pd.Timedelta('1w')]\n",
    "    merged_notes = Notes_included.groupby('NOTE_ID')['NOTE_TEXT'].agg(' '.join)\n",
    "    if merged_notes.shape[0] == 0:\n",
    "        continue\n",
    "    Notes_included = pd.merge(Notes_included, merged_notes, on='NOTE_ID',how='left')\n",
    "    Notes_included.drop('NOTE_TEXT_x',axis=1,inplace=True)\n",
    "    Notes_included.drop_duplicates('NOTE_TEXT_y',inplace=True)\n",
    "    Notes_included['line length'] = Notes_included['NOTE_TEXT_y'].apply(len)\n",
    "    Notes_included = Notes_included.iloc[::-1]\n",
    "    Notes_included['length sum'] = Notes_included['line length'].cumsum()\n",
    "    Notes_included['met threshold?'] = Notes_included['length sum'] >= cutoff\n",
    "    if Notes_included['met threshold?'].sum() == 0:\n",
    "        row_index = Notes_included.shape[0]\n",
    "    else:\n",
    "        row_index = Notes_included['met threshold?'].argmax()\n",
    "    num_notes.append(row_index+1)\n",
    "    Notes_included = Notes_included.iloc[:row_index+1]\n",
    "    Notes_included['time_dif'] = scantime - Notes_included['Entry DateTime']\n",
    "    Notes_included['word_count'] = Notes_included['NOTE_TEXT_y'].apply(lambda text: len(text.split()))\n",
    "    word_counts.append(Notes_included['word_count'].sum())\n",
    "    char_counts.append(Notes_included['line length'].sum())\n",
    "    time_difs.append(Notes_included['time_dif'])\n",
    "\n",
    "time_difs = pd.concat(time_difs)\n",
    "minutes = time_difs.dt.total_seconds()/60\n",
    "minutes.sort_values(inplace=True)\n",
    "\n",
    "print('Median minutes between included notes and scan time [IQR]')\n",
    "print(np.percentile(minutes,25))\n",
    "print(np.percentile(minutes,50))\n",
    "print(np.percentile(minutes,75))\n",
    "\n",
    "plt.hist(word_counts,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median word count in included text [IQR]')\n",
    "print(np.percentile(word_counts,25))\n",
    "print(np.percentile(word_counts,50))\n",
    "print(np.percentile(word_counts,75))\n",
    "\n",
    "plt.hist(char_counts,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median char count in included text [IQR]')\n",
    "print(np.percentile(char_counts,25))\n",
    "print(np.percentile(char_counts,50))\n",
    "print(np.percentile(char_counts,75))\n",
    "\n",
    "plt.hist(num_notes,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median num notes count in included text [IQR]')\n",
    "print(np.percentile(num_notes,25))\n",
    "print(np.percentile(num_notes,50))\n",
    "print(np.percentile(num_notes,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5000\n",
    "num_notes = []\n",
    "time_difs = []\n",
    "word_counts = []\n",
    "char_counts = []\n",
    "\n",
    "for id,scantime in IAD[['PatientID','ScanTime']].values:\n",
    "    Notes_included = Notes[Notes['PAT_MRN_ID']== str(id)]\n",
    "    Notes_included['Entry DateTime'] = Notes_included['ENTRY_INST_LOCAL_DTTM'].apply(pd.to_datetime)\n",
    "    scantime = pd.to_datetime(scantime)\n",
    "    Notes_included = Notes_included[scantime > Notes_included['Entry DateTime']]\n",
    "    Notes_included = Notes_included[scantime - Notes_included['Entry DateTime'] < pd.Timedelta('1w')]\n",
    "    merged_notes = Notes_included.groupby('NOTE_ID')['NOTE_TEXT'].agg(' '.join)\n",
    "    if merged_notes.shape[0] == 0:\n",
    "        continue\n",
    "    Notes_included = pd.merge(Notes_included, merged_notes, on='NOTE_ID',how='left')\n",
    "    Notes_included.drop('NOTE_TEXT_x',axis=1,inplace=True)\n",
    "    Notes_included.drop_duplicates('NOTE_TEXT_y',inplace=True)\n",
    "    Notes_included['line length'] = Notes_included['NOTE_TEXT_y'].apply(len)\n",
    "    Notes_included = Notes_included.iloc[::-1]\n",
    "    Notes_included['length sum'] = Notes_included['line length'].cumsum()\n",
    "    Notes_included['met threshold?'] = Notes_included['length sum'] >= cutoff\n",
    "    if Notes_included['met threshold?'].sum() == 0:\n",
    "        row_index = Notes_included.shape[0]\n",
    "    else:\n",
    "        row_index = Notes_included['met threshold?'].argmax()\n",
    "    num_notes.append(row_index+1)\n",
    "    Notes_included = Notes_included.iloc[:row_index+1]\n",
    "    Notes_included['time_dif'] = scantime - Notes_included['Entry DateTime']\n",
    "    Notes_included['word_count'] = Notes_included['NOTE_TEXT_y'].apply(lambda text: len(text.split()))\n",
    "    word_counts.append(Notes_included['word_count'].sum())\n",
    "    char_counts.append(Notes_included['line length'].sum())\n",
    "    time_difs.append(Notes_included['time_dif'])\n",
    "\n",
    "time_difs = pd.concat(time_difs)\n",
    "minutes = time_difs.dt.total_seconds()/60\n",
    "minutes.sort_values(inplace=True)\n",
    "\n",
    "print('Median minutes between included notes and scan time [IQR]')\n",
    "print(np.percentile(minutes,25))\n",
    "print(np.percentile(minutes,50))\n",
    "print(np.percentile(minutes,75))\n",
    "\n",
    "plt.hist(word_counts,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median word count in included text [IQR]')\n",
    "print(np.percentile(word_counts,25))\n",
    "print(np.percentile(word_counts,50))\n",
    "print(np.percentile(word_counts,75))\n",
    "\n",
    "plt.hist(char_counts,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median char count in included text [IQR]')\n",
    "print(np.percentile(char_counts,25))\n",
    "print(np.percentile(char_counts,50))\n",
    "print(np.percentile(char_counts,75))\n",
    "\n",
    "plt.hist(num_notes,30)\n",
    "plt.show()\n",
    "\n",
    "print('Median num notes count in included text [IQR]')\n",
    "print(np.percentile(num_notes,25))\n",
    "print(np.percentile(num_notes,50))\n",
    "print(np.percentile(num_notes,75))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
