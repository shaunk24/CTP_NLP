{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from comorbidipy import comorbidity\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = ' (cutoff 5000)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv('Unprocessed Text (120)' + cutoff,header=0)\n",
    "text.rename(columns={'Unnamed: 0':'PatientID'},inplace=True)\n",
    "text['length'] = text['text'].apply(len)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = text['length'].max()\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.max_length = z\n",
    "\n",
    "def initial_clean(text):\n",
    "     \n",
    "    doc = nlp(text)\n",
    "    text = [token.text.lower() for token in doc]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    text = ' '.join([token for token in text.split() if token not in stop_words])\n",
    "\n",
    "    # Remove punctuations\n",
    "    text = ' '.join([token for token in text.split() if token.isalpha()])\n",
    "\n",
    "    # Perform stemming or lemmatization\n",
    "    text = ' '.join([token.lemma_ for token in nlp(text)])\n",
    "    return(text)\n",
    "\n",
    "text['text']= text['text'].apply(initial_clean)\n",
    "text.to_csv('Processed Text (120)'+ cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text['length'] = text['text'].apply(len)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2)\n",
    "X = vectorizer.fit_transform(text['text'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df.to_csv('TFIDF (120)' + cutoff)\n",
    "\n",
    "def get_top_words(document_index, n):\n",
    "    doc_tfidf_scores = tfidf_df.iloc[document_index]\n",
    "    top_indices = np.argsort(doc_tfidf_scores)[-n:][::-1]\n",
    "    top_words = [vectorizer.get_feature_names_out()[i] for i in top_indices]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_words(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA = pd.read_csv('Included Aggregate Data (120)',index_col=0)\n",
    "ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_mapping = {\n",
    "    'M1': 'MCA',\n",
    "    'P2': 'PCA',\n",
    "    'M2': 'MCA',\n",
    "    'C-ICA': 'ICA',\n",
    "    'ICA-M1 tandem': 'ICA',\n",
    "    'P3': 'PCA',\n",
    "    'top-basilar': 'Basilar/top vertebral',\n",
    "    'T-ICA': 'ICA',\n",
    "    'ICA-M2 tandem': 'ICA',\n",
    "    'A2': 'ACA',\n",
    "    'P1': 'PCA',\n",
    "    'M4': 'MCA',\n",
    "    'M3': 'MCA',\n",
    "    'ICA->=M3 tandem': 'ICA',\n",
    "    'pre-terminal I-ICA': 'ICA',\n",
    "    'VA': 'Basilar/top vertebral',\n",
    "    'A1': 'ACA'\n",
    "}\n",
    "\n",
    "ISA['Occlusion Location'] = ISA['Primary Arterial Site'].map(stroke_mapping)\n",
    "ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demographics = pd.read_excel('CTP_demographics_first_151_patients_3-14-23.xlsx',header=0)\n",
    "History = pd.read_excel('CTP_patient_med_hx_w_UserEnteredDates_first_151_patients_3-28-23.xlsx',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.get_dummies(Demographics,columns=['Sex', 'PreferredLanguage','English_NonEnglish', 'FirstRace', 'SecondRace', 'PostalCode'])\n",
    "D['BirthDate'] = pd.to_datetime(D['BirthDate'])\n",
    "D['PrimaryMrn']= D['PrimaryMrn'].apply(str)\n",
    "D=D[D['PrimaryMrn'].isin(ISA['PatientID'])]\n",
    "\n",
    "M2 = pd.merge(ISA,D,left_on='PatientID', right_on='PrimaryMrn').drop(['PrimaryMrn'],axis=1)\n",
    "\n",
    "H = History[History['UserEnteredDateKey']!= -1]\n",
    "H['Date'] = pd.to_datetime(H['UserEnteredDateKey'].apply(str),format='%Y%m%d')\n",
    "H['PrimaryMrn'] = H['PrimaryMrn'].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_times=M2[['PatientID','ScanTime']].drop_duplicates()\n",
    "IDs = []\n",
    "codes = []\n",
    "for i in range(scan_times.shape[0]):\n",
    "    patient_mrn = scan_times['PatientID'].values[i]\n",
    "    patient_scan = scan_times['ScanTime'].values[i]\n",
    "    patient_rows = H[H['PrimaryMrn']==patient_mrn]\n",
    "    good_rows = patient_rows[pd.to_datetime(patient_scan)-patient_rows['Date']> pd.Timedelta('0m') ]\n",
    "    if good_rows.empty:\n",
    "        continue\n",
    "    for i,row in good_rows.iterrows():\n",
    "        ICDs = row[pd.notna(row)].values\n",
    "        for code in ICDs:\n",
    "            IDs.append(patient_mrn)\n",
    "            codes.append(str(code))\n",
    "ICD = pd.DataFrame()\n",
    "ICD['id'] = IDs\n",
    "ICD['code'] = codes\n",
    "COMO = comorbidity(df=ICD,age=None,variant='quan',weighting='vw',score = 'elixhauser')\n",
    "COMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMO.to_csv('Comorbidities (80)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = pd.merge(M2,COMO[['comorbidity_score','id']], how='left', left_on='PatientID',right_on='id').drop(['id'],axis=1)\n",
    "F['AgeAsofToday'] = F['ScanTime'].apply(pd.to_datetime) - F['BirthDate']\n",
    "F['AgeAsofToday'] = F['AgeAsofToday'].astype(int)/3.154e16\n",
    "F['LKWDiff'] = F['LKWDiff'].apply(pd.to_timedelta).astype(int)/3.6e12\n",
    "F = F.rename(columns={'AgeAsofToday':'AgeAsofScan'}).drop(['ScanTime','BirthDate','IsCurrent'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.to_csv('IAD With Hist and Demos (120)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load('/Users/shaunkohli/Desktop/Kummer Project/word2vec_model.kv', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_Vecs = np.zeros([tfidf_df.shape[1],200])\n",
    "for i,word in enumerate(tfidf_df.columns):\n",
    "    if word in word2vec_model.key_to_index:\n",
    "        word_Vecs[i] = word2vec_model[word]\n",
    "    else:\n",
    "        word_Vecs[i] = np.zeros(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array = np.array(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_vecs = np.matmul(tfidf_array,word_Vecs)\n",
    "weighted_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_vecs = pd.DataFrame(weighted_vecs)\n",
    "weighted_vecs.to_csv('BioWord2Vecs (120)' +cutoff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
