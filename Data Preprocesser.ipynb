{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregate = pd.read_excel('aggregate_20010101-20220314_annotated_4-11-22 (2).xlsx',header=1)\n",
    "Notes = pd.read_excel('CTP_ClarityNotes_N=151_v3.xlsx',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRNs = pd.read_csv('Included MRNs (120)',header=0,index_col=0)['PatientID'].map(str)\n",
    "MRNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregate['PatientID'] = Aggregate['PatientID'].map(str)\n",
    "Aggregate_Included = Aggregate[Aggregate['PatientID'].isin(MRNs)]\n",
    "Aggregate_Included_CTP = Aggregate_Included[Aggregate_Included['StudyDescription'].str.contains('P', case=False, na=False)]\n",
    "\n",
    "Aggregate_Included_CTP_WithRatio = Aggregate_Included_CTP[pd.notna(Aggregate_Included_CTP['P:C(30)>=1.8'])]\n",
    "Aggregate_Included_CTP_WithRatio_AndPenumbra = Aggregate_Included_CTP_WithRatio[Aggregate_Included_CTP_WithRatio['>6s']!= 0]\n",
    "Included_Subjects_Aggregate = Aggregate_Included_CTP_WithRatio_AndPenumbra.sort_values(['StudyDate','StudyTime']).drop_duplicates('PatientID').sort_index()\n",
    "\n",
    "Included_Subjects_Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aggregate_Included_WithRatio = Aggregate_Included[pd.notna(Aggregate_Included['P:C(30)>=1.8'])]\n",
    "Aggregate_Included_WithRatio_AndPenumbra = Aggregate_Included_WithRatio[Aggregate_Included_WithRatio['>6s']!= 0]\n",
    "Included_Subjects_FirstScanTimes = Aggregate_Included_WithRatio_AndPenumbra.sort_values(['StudyDate','StudyTime']).drop_duplicates('PatientID').sort_index()[['PatientID','StudyDate','StudyTime','StudyDescription']]\n",
    "Included_Subjects_FirstScanTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(lkwTime):\n",
    "    x = str(int(lkwTime))\n",
    "    while len(x)<6:\n",
    "        x = '0' + x\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Included_Subjects_Aggregate['ScanTime'] = Included_Subjects_Aggregate['StudyDate'].apply(pad) + Included_Subjects_Aggregate['StudyTime'].apply(pad)\n",
    "Included_Subjects_Aggregate['ScanTime'] = pd.to_datetime(Included_Subjects_Aggregate['ScanTime'],format='%Y%m%d%H%M%S') \n",
    "Included_Subjects_Aggregate['LKWTime'] = Included_Subjects_Aggregate['LKW date'].apply(pad) + Included_Subjects_Aggregate['LKW time'].apply(pad)\n",
    "Included_Subjects_Aggregate['LKWTime'] = pd.to_datetime(Included_Subjects_Aggregate['LKWTime'],format='%Y%m%d%H%M%S') \n",
    "Included_Subjects_Aggregate['LKWDiff'] = Included_Subjects_Aggregate['ScanTime'] - Included_Subjects_Aggregate['LKWTime']\n",
    "Included_Subjects_Aggregate = Included_Subjects_Aggregate.drop(['StudyDate', 'StudyTime','LKW time','LKW date','LKWTime'],axis=1)\n",
    "Included_Subjects_Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Included_Subjects_FirstScanTimes['ScanTime'] = Included_Subjects_FirstScanTimes['StudyDate'].apply(pad) + Included_Subjects_FirstScanTimes['StudyTime'].apply(pad)\n",
    "Included_Subjects_FirstScanTimes['ScanTime'] = pd.to_datetime(Included_Subjects_FirstScanTimes['ScanTime'],format='%Y%m%d%H%M%S') \n",
    "Included_Subjects_FirstScanTimes = Included_Subjects_FirstScanTimes.drop(['StudyDate','StudyTime'], axis=1)\n",
    "Included_Subjects_FirstScanTimes.columns = ['PatientID', 'FirstScanType', 'FirstScanTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Included_Subjects_FirstScanTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = Included_Subjects_Aggregate.merge(\n",
    "    Included_Subjects_FirstScanTimes[['PatientID', 'FirstScanType', 'FirstScanTime']],\n",
    "    on='PatientID',\n",
    "    how='left'  # Use 'inner' if you only want to keep rows that have matching PatientID in both dataframes\n",
    ")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA = merged_df[['PatientID', '>4s', '>6s', '>8s', '>10s', '<38%', '<34%', '<30%', '<20%',\n",
    "                    'CBF30_adj_0cc', 'P:C(30)>=1.8', 'P-C', 'CBF38_adj_0cc', 'P:C(38)>=1.8',\n",
    "                    'PatientBirthDate', 'Lat', 'FirstScanType','FirstScanTime','ScanTime',\n",
    "                    'LKWDiff']]\n",
    "ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arterial_sites = Aggregate[~pd.isna(Aggregate['Primary Arterial Site'])]\n",
    "arterial_sites = arterial_sites[arterial_sites['PatientID'].isin(ISA['PatientID'].values)].drop_duplicates('PatientID').sort_index()\n",
    "arterial_sites = arterial_sites[['PatientID','Primary Arterial Site']]\n",
    "arterial_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA = pd.merge(ISA,arterial_sites,left_on='PatientID',right_on='PatientID')\n",
    "ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA['PatientID'] = ISA['PatientID'].apply(str)\n",
    "ISA['PatientBirthDate'] = ISA['PatientBirthDate'] .apply(int)\n",
    "ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charlen_total_notes = []\n",
    "charlen_first_notes = []\n",
    "\n",
    "for id,scantime in ISA[['PatientID','ScanTime']].values:\n",
    "    Notes_included = Notes[Notes['PAT_MRN_ID']== str(id)]\n",
    "    Notes_included['Entry DateTime'] = Notes_included['ENTRY_INST_LOCAL_DTTM'].apply(pd.to_datetime)\n",
    "    scantime = pd.to_datetime(scantime)\n",
    "    Notes_included = Notes_included[scantime > Notes_included['Entry DateTime']]\n",
    "    Notes_included = Notes_included[scantime - Notes_included['Entry DateTime'] < pd.Timedelta('1w')]\n",
    "    merged_notes = Notes_included.groupby('NOTE_ID')['NOTE_TEXT'].agg(' '.join)\n",
    "    if merged_notes.shape[0] == 0:\n",
    "        continue\n",
    "    Notes_included = pd.merge(Notes_included, merged_notes, on='NOTE_ID',how='left')\n",
    "    Notes_included.drop('NOTE_TEXT_x',axis=1,inplace=True)\n",
    "    Notes_included.drop_duplicates('NOTE_TEXT_y',inplace=True)\n",
    "    Notes_included['line length'] = Notes_included['NOTE_TEXT_y'].apply(len)\n",
    "    Notes_included = Notes_included.iloc[::-1].reset_index()\n",
    "    total_Note = ' '.join(Notes_included['NOTE_TEXT_y'].values)\n",
    "    first_Note = Notes_included['NOTE_TEXT_y'][0]\n",
    "    charlen_first_notes.append(len(first_Note))\n",
    "    charlen_total_notes.append(len(total_Note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dist of total note lengths in week prior to scan')\n",
    "print(np.percentile(charlen_total_notes,25))\n",
    "print(np.percentile(charlen_total_notes,50))\n",
    "print(np.percentile(charlen_total_notes,75))\n",
    "\n",
    "plt.hist(charlen_total_notes,30)\n",
    "plt.show()\n",
    "\n",
    "print('dist of first note lengths in week prior to scan')\n",
    "print(np.percentile(charlen_first_notes,25))\n",
    "print(np.percentile(charlen_first_notes,50))\n",
    "print(np.percentile(charlen_first_notes,75))\n",
    "\n",
    "plt.hist(charlen_first_notes,30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [500,1000,5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    text = dict()\n",
    "    for id,scantime in ISA[['PatientID','ScanTime']].values:\n",
    "        Notes_included = Notes[Notes['PAT_MRN_ID']== str(id)]\n",
    "        Notes_included['Entry DateTime'] = Notes_included['ENTRY_INST_LOCAL_DTTM'].apply(pd.to_datetime)\n",
    "        Notes_included = Notes_included[scantime > Notes_included['Entry DateTime']]\n",
    "        Notes_included = Notes_included[scantime - Notes_included['Entry DateTime'] < pd.Timedelta('1w')]\n",
    "        merged_notes = Notes_included.groupby('NOTE_ID')['NOTE_TEXT'].agg(' '.join)\n",
    "        if merged_notes.shape[0] == 0:\n",
    "            continue\n",
    "        Notes_included = pd.merge(Notes_included, merged_notes, on='NOTE_ID',how='left')\n",
    "        Notes_included.drop('NOTE_TEXT_x',axis=1,inplace=True)\n",
    "        Notes_included.drop_duplicates('NOTE_TEXT_y',inplace=True)\n",
    "        Notes_included['line length'] = Notes_included['NOTE_TEXT_y'].apply(len)\n",
    "        Notes_included = Notes_included.iloc[::-1]\n",
    "        Notes_included['length sum'] = Notes_included['line length'].cumsum()\n",
    "        Notes_included['met threshold?'] = Notes_included['length sum'] >= cutoff\n",
    "        if Notes_included['met threshold?'].sum() == 0:\n",
    "            row_index = Notes_included.shape[0]\n",
    "        else:\n",
    "            row_index = Notes_included['met threshold?'].argmax()\n",
    "        Notes_included = Notes_included.iloc[:row_index+1]\n",
    "        Note = ' '.join(Notes_included['NOTE_TEXT_y'].values)\n",
    "        text[id] = Note\n",
    "\n",
    "    Text = pd.DataFrame(text.values(),index=text.keys(),columns=['text'])\n",
    "    Text.to_csv('Unprocessed Text (120) (cutoff ' + str(cutoff) + ')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISA.to_csv('Included Aggregate Data (120)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
